{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.color_palette(\"muted\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle as pkl\n",
    "from collections import defaultdict\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of variables to be excluded for logistic regression\n",
    "# these must be the first two variables in the feature matrix\n",
    "\n",
    "# global inputs\n",
    "logit_num = 4\n",
    "model_names = ['l1','l2']\n",
    "\n",
    "'''\n",
    "Model Building\n",
    "'''\n",
    "\n",
    "# Pipeline dictionary\n",
    "pipelines = {\n",
    "    'l1' : make_pipeline(StandardScaler(), LogisticRegression( penalty = 'l1', random_state=125)),\n",
    "    'l2' : make_pipeline(StandardScaler(), LogisticRegression( penalty = 'l2', random_state=125)),\n",
    "    'rf' : make_pipeline(StandardScaler(), RandomForestClassifier(random_state=125)),\n",
    "    'gb' : make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=125)),\n",
    "    'linsvc' : make_pipeline(StandardScaler(), SVC(random_state=125,probability=True)),\n",
    "    'rbfsvc' : make_pipeline(StandardScaler(), SVC(random_state=125,probability=True))\n",
    "}\n",
    "\n",
    "# Logistic Regression hyperparameters\n",
    "l1_hyperparameters = {\n",
    "    'logisticregression__C' : np.linspace(1e-3, 1e2, 500)\n",
    "}\n",
    "\n",
    "l2_hyperparameters = {\n",
    "    'logisticregression__C' : np.linspace(1e-3, 1e2, 500)\n",
    "}\n",
    "\n",
    "# Random Forest hyperparameters\n",
    "rf_hyperparameters = {\n",
    "    'randomforestclassifier__n_estimators': [100, 300, 500],\n",
    "    'randomforestclassifier__max_features': ['auto', 'sqrt', 0.33],\n",
    "    'randomforestclassifier__max_depth': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Boosted Tree hyperparameters\n",
    "gb_hyperparameters = {\n",
    "    'gradientboostingclassifier__n_estimators': [100, 300, 500],\n",
    "    'gradientboostingclassifier__learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'gradientboostingclassifier__max_depth': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "linsvc_hyperparameters = {\n",
    "    'svc__C' : [1e-5, 1e-3, 1e-1, 1e1],\n",
    "    'svc__kernel' : ['linear']\n",
    "}\n",
    "\n",
    "rbfsvc_hyperparameters = {\n",
    "    'svc__C': [1e-5, 1e-3, 1e-1, 1e1],\n",
    "    'svc__gamma' : [1e-5, 1e-3, 1e-1, 1e1],\n",
    "    'svc__kernel' : ['rbf']\n",
    "}\n",
    "# Create hyperparameters dictionary\n",
    "hyperparameters = {\n",
    "    'l1' : l1_hyperparameters, \n",
    "    'l2' : l2_hyperparameters,\n",
    "    'rf' : rf_hyperparameters,\n",
    "    'gb' : gb_hyperparameters,\n",
    "    'linsvc' : linsvc_hyperparameters,\n",
    "    'rbfsvc' : rbfsvc_hyperparameters\n",
    "}\n",
    "# Create data pointing dictionary\n",
    "datapointers = {\n",
    "    'l1' : 'logistic',\n",
    "    'l2' : 'logistic',\n",
    "    'rf' : 'not_logistic',\n",
    "    'gb' : 'not_logistic',\n",
    "    'linsvc' : 'not logistic',\n",
    "    'rbfsvc' : 'not logistic'\n",
    "}\n",
    "\n",
    "def model_scoring_auc(X_in, y_in, model, datapointer):\n",
    "    if datapointer == 'logistic':\n",
    "        pred = model.predict_proba(X_in[:,logit_num:])\n",
    "    else:\n",
    "        pred = model.predict_proba(X_in)\n",
    "    # Get just the prediction for the positive class (1)\n",
    "    pred = [p[1] for p in pred]\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_in, pred)\n",
    "    # Calculate AUROC\n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "\n",
    "def model_fitting(df, y, logit_num, model_names, pipelines, hyperparameters, datapointers, randstate):\n",
    "    # Create empty dictionary called fitted_models\n",
    "    fitted_models = {}\n",
    "    fitted_scores = {}\n",
    "    \n",
    "    # split data for CV testing\n",
    "    \n",
    "    #this works:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=randstate,stratify=X[:,9])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "    for name in model_names:\n",
    "        # Create cross-validation object from pipeline and hyperparameters\n",
    "        model = GridSearchCV(pipelines[name], hyperparameters[name], scoring = 'neg_log_loss', cv=10, refit=True)\n",
    "\n",
    "        # Fit model on X_train, y_train\n",
    "        if datapointers[name] == 'logistic':\n",
    "            model.fit(X_train[:,logit_num:], y_train)  \n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        # Store model in fitted_models[name] \n",
    "        fitted_models[name] = model\n",
    "        \n",
    "        # store scores in fitted_scores[name]\n",
    "        train_score = model_scoring_auc(X_train, y_train, model, datapointers[name])\n",
    "        test_score = model_scoring_auc(X_test, y_test, model, datapointers[name])\n",
    "        fitted_scores[name] = [train_score,test_score]\n",
    "            \n",
    "    return fitted_models, fitted_scores\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l1': [0.7911265232045631, 0.7697101449275363], 'l2': [0.7908024371273009, 0.7671014492753623]}\n",
      "{'l1': [0.7767009110292692, 0.789906103286385], 'l2': [0.7768462880403177, 0.7910798122065728]}\n",
      "{'l1': [0.7916420361247948, 0.7770232031692135], 'l2': [0.7910344827586206, 0.7770232031692134]}\n",
      "{'l1': [0.7925110982435822, 0.7325825825825826], 'l2': [0.7928327864633598, 0.7373873873873874]}\n",
      "{'l1': [0.792640566160239, 0.7779700115340255], 'l2': [0.7925756395273341, 0.7759515570934256]}\n",
      "{'l1': [0.8087323124636557, 0.7241784037558685], 'l2': [0.8079892744071849, 0.7262323943661971]}\n",
      "{'l1': [0.8001385130782115, 0.7331745086360929], 'l2': [0.8001546192500967, 0.7313877307921381]}\n",
      "{'l1': [0.7691371250486949, 0.8347750865051905], 'l2': [0.7681307622386703, 0.8393886966551327]}\n",
      "{'l1': [0.8093333333333332, 0.7072330654420207], 'l2': [0.8090081300813008, 0.7152698048220436]}\n",
      "{'l1': [0.8038861788617886, 0.7223019517795638], 'l2': [0.8025853658536586, 0.7276119402985075]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('../data/fitting_data.csv')\n",
    "df.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "df = df.fillna(0)\n",
    "y = df.pop('music').values\n",
    "X = df.values\n",
    "\n",
    "\n",
    "models_iterate = {}\n",
    "scores_iterate = {}\n",
    "for i in range(100):\n",
    "    models_iterate[i], scores_iterate[i] = model_fitting(X,y,logit_num,model_names,pipelines,hyperparameters,datapointers,i)\n",
    "    print(scores_iterate[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('../data/100models.pkl', 'wb') as picklefile:\n",
    "    pkl.dump(models_iterate, picklefile)\n",
    "with open('../data/100models_scores.pkl', 'wb') as picklefile:\n",
    "    pkl.dump(scores_iterate, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
